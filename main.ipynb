{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d399be1b",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24407a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d4848\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import logging\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from tkinter import Tk, filedialog\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e370b29",
   "metadata": {},
   "source": [
    "Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23af239",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"support_bot_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14345f",
   "metadata": {},
   "source": [
    "Creating a Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a57911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportBotAgent:\n",
    "    def __init__(self, use_embeddings=True, relevance_threshold=0.5):\n",
    "        self.doc_text = \"\"\n",
    "        self.sentences = []\n",
    "        self.doc_embeddings = None\n",
    "        self.use_embeddings = use_embeddings\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\") if use_embeddings else None\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "        self.gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "        self.gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "        self.generator = pipeline(\"text-generation\", model=self.gpt2_model, tokenizer=self.gpt2_tokenizer)\n",
    "        logging.info(\"SupportBotAgent initialized.\")\n",
    "\n",
    "    # Load Document\n",
    "    def load_document(self, path):\n",
    "        text = \"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    page_text = \" \".join(page_text.split())  # clean line breaks\n",
    "                    text += page_text + \" \"\n",
    "        self.doc_text = text.strip()\n",
    "\n",
    "        if self.use_embeddings and text.strip():\n",
    "            sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "            self.sentences = sentences\n",
    "            self.doc_embeddings = self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        logging.info(\"Document loaded: %s with %d characters\", path, len(text))\n",
    "\n",
    "    # Answer Query\n",
    "    def answer_query(self, query):\n",
    "        logging.info(\"Query received: %s\", query)   # log query\n",
    "        if not self.doc_text.strip():\n",
    "            return \"No document loaded.\"\n",
    "\n",
    "        if not self.use_embeddings:\n",
    "            return self.doc_text[:200] + \"...\"\n",
    "\n",
    "    # Encode query & find best context\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        cos_scores = util.cos_sim(query_embedding, self.doc_embeddings)[0]\n",
    "        top_score, top_idx = torch.max(cos_scores, dim=0)\n",
    "\n",
    "    # Relevance check\n",
    "        if top_score < self.relevance_threshold:\n",
    "            return \"I donâ€™t have enough information to answer that.\"\n",
    "\n",
    "    # Use QA model\n",
    "        context = self.sentences[top_idx]\n",
    "        qa_result = self.qa_pipeline(question=query, context=context)\n",
    "        answer = qa_result[\"answer\"].strip()\n",
    "\n",
    "    # If answer is too short, use GPT-2 to expand it\n",
    "        if len(answer) < 20:\n",
    "            prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "            gpt2_output = self.generator(\n",
    "            prompt,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # Extract only the answer portion\n",
    "            if \"Answer:\" in gpt2_output:\n",
    "                gpt2_answer = gpt2_output.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                gpt2_answer = gpt2_output.strip()\n",
    "\n",
    "            return gpt2_answer\n",
    "\n",
    "    # If answer is already good length, just return it\n",
    "        return answer\n",
    "\n",
    "\n",
    "    # Simulate Feedback\n",
    "    def simulate_feedback(self, response):\n",
    "        feedback_options = [\"not helpful\", \"too vague\", \"good\"]\n",
    "        feedback = random.choice(feedback_options)\n",
    "\n",
    "        if feedback == \"not helpful\":\n",
    "            response = response + \" (rephrased for clarity)\"\n",
    "        elif feedback == \"too vague\":\n",
    "            response = response + \" (added more context)\"\n",
    "        # \"good\" -> keep as is\n",
    "\n",
    "        logging.info(\"Feedback: %s | Adjusted Response: %s\", feedback, response)\n",
    "        return response, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c410fc2",
   "metadata": {},
   "source": [
    "Main Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5543f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:04<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.76it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: Serri AI will replace the existing system of servers and make it easy to extend your games in any direction. Serri AI will make server managment much much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managing much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server manag\n",
      "ðŸ”„ Iteration 1 | Feedback: too vague | New Answer: Serri AI will replace the existing system of servers and make it easy to extend your games in any direction. Serri AI will make server managment much much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managing much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server manag (added more context)\n",
      "ðŸ”„ Iteration 2 | Feedback: good | New Answer: Serri AI will replace the existing system of servers and make it easy to extend your games in any direction. Serri AI will make server managment much much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managing much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server managment much more efficient. Serri AI will make server manag (added more context)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: AI agents work with your existing sales, marketing, and support systems\n",
      "ðŸ”„ Iteration 1 | Feedback: not helpful | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity)\n",
      "ðŸ”„ Iteration 2 | Feedback: not helpful | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 3 | Feedback: not helpful | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 4 | Feedback: not helpful | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 5 | Feedback: too vague | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (added more context)\n",
      "ðŸ”„ Iteration 6 | Feedback: too vague | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (added more context) (added more context)\n",
      "ðŸ”„ Iteration 7 | Feedback: not helpful | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (added more context) (added more context) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 8 | Feedback: not helpful | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (added more context) (added more context) (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 9 | Feedback: good | New Answer: AI agents work with your existing sales, marketing, and support systems (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (rephrased for clarity) (added more context) (added more context) (rephrased for clarity) (rephrased for clarity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: I donâ€™t have enough information to answer that.\n",
      "ðŸ”„ Iteration 1 | Feedback: not helpful | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity)\n",
      "ðŸ”„ Iteration 2 | Feedback: not helpful | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 3 | Feedback: not helpful | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity) (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 4 | Feedback: good | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity) (rephrased for clarity) (rephrased for clarity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: I donâ€™t have enough information to answer that.\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: I donâ€™t have enough information to answer that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.40it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: The AI consists of a variety of elements to be able to solve problems in game development. The AI consists of:\n",
      "- A player called Serri AI who will have a set of AI functions. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- An AI called Serri AI that is capable of learning from experience and adapts to the environment.\n",
      "- A player called Serri AI that is capable of creating a character named Serri AI. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- A player called Serri AI that is able to create a world and map called Serri AI world map. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "ðŸ”„ Iteration 1 | Feedback: not helpful | New Answer: The AI consists of a variety of elements to be able to solve problems in game development. The AI consists of:\n",
      "- A player called Serri AI who will have a set of AI functions. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- An AI called Serri AI that is capable of learning from experience and adapts to the environment.\n",
      "- A player called Serri AI that is capable of creating a character named Serri AI. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- A player called Serri AI that is able to create a world and map called Serri AI world map. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based. (rephrased for clarity)\n",
      "ðŸ”„ Iteration 2 | Feedback: too vague | New Answer: The AI consists of a variety of elements to be able to solve problems in game development. The AI consists of:\n",
      "- A player called Serri AI who will have a set of AI functions. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- An AI called Serri AI that is capable of learning from experience and adapts to the environment.\n",
      "- A player called Serri AI that is capable of creating a character named Serri AI. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- A player called Serri AI that is able to create a world and map called Serri AI world map. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based. (rephrased for clarity) (added more context)\n",
      "ðŸ”„ Iteration 3 | Feedback: good | New Answer: The AI consists of a variety of elements to be able to solve problems in game development. The AI consists of:\n",
      "- A player called Serri AI who will have a set of AI functions. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- An AI called Serri AI that is capable of learning from experience and adapts to the environment.\n",
      "- A player called Serri AI that is capable of creating a character named Serri AI. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based.\n",
      "- A player called Serri AI that is able to create a world and map called Serri AI world map. The function that you choose is based on the type of the game, such as action based, puzzle based or combat based. (rephrased for clarity) (added more context)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: I donâ€™t have enough information to answer that.\n",
      "ðŸ”„ Iteration 1 | Feedback: not helpful | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity)\n",
      "ðŸ”„ Iteration 2 | Feedback: not helpful | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 3 | Feedback: too vague | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity) (rephrased for clarity) (added more context)\n",
      "ðŸ”„ Iteration 4 | Feedback: good | New Answer: I donâ€™t have enough information to answer that. (rephrased for clarity) (rephrased for clarity) (added more context)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: I donâ€™t have enough information to answer that.\n",
      "ðŸ”„ Iteration 1 | Feedback: too vague | New Answer: I donâ€™t have enough information to answer that. (added more context)\n",
      "ðŸ”„ Iteration 2 | Feedback: good | New Answer: I donâ€™t have enough information to answer that. (added more context)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: Inform visitors about Serri's features and benefits\n",
      "ðŸ”„ Iteration 1 | Feedback: too vague | New Answer: Inform visitors about Serri's features and benefits (added more context)\n",
      "ðŸ”„ Iteration 2 | Feedback: not helpful | New Answer: Inform visitors about Serri's features and benefits (added more context) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 3 | Feedback: good | New Answer: Inform visitors about Serri's features and benefits (added more context) (rephrased for clarity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: Target Industries & Use Cases\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: Target Industries & Use Cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: Target Industries & Use Cases\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: Target Industries & Use Cases\n",
      "\n",
      "ðŸ™ Thanks for using the Support Bot!\n",
      "âœ¨ Hope I was able to help you today.\n",
      "ðŸ’¬ Goodbye, and have a great day ahead! ðŸš€\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = SupportBotAgent()\n",
    "\n",
    "    # File picker: choose ANY PDF from your PC\n",
    "    Tk().withdraw()  # hide empty Tk window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a PDF file\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\")]\n",
    "    )\n",
    "\n",
    "    if not file_path:\n",
    "        print(\"âŒ No file selected.\")\n",
    "    else:\n",
    "        agent.load_document(file_path)\n",
    "        print(\"âœ… Document loaded successfully!\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"\\nAsk a question (or type 'exit'): \")\n",
    "            if query.lower() == \"exit\":\n",
    "                print(\"\\nðŸ™ Thanks for using the Support Bot!\")\n",
    "                print(\"âœ¨ Hope I was able to help you today.\")\n",
    "                print(\"ðŸ’¬ Goodbye, and have a great day ahead! ðŸš€\\n\")\n",
    "                break\n",
    "\n",
    "    # Step 1: Answer query\n",
    "            answer = agent.answer_query(query)\n",
    "            print(\"ðŸ’¡ Answer:\", answer)\n",
    "\n",
    "    # Step 2: Refine with feedback (up to 10 iterations)\n",
    "            for i in range(10):\n",
    "                adjusted, fb = agent.simulate_feedback(answer)\n",
    "                print(f\"ðŸ”„ Iteration {i+1} | Feedback: {fb} | New Answer: {adjusted}\")\n",
    "                answer = adjusted\n",
    "                if fb == \"good\":\n",
    "                    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
