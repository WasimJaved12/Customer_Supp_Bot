{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d399be1b",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24407a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import logging\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from tkinter import Tk, filedialog\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e370b29",
   "metadata": {},
   "source": [
    "Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b23af239",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"support_bot_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14345f",
   "metadata": {},
   "source": [
    "Creating a Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a57911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportBotAgent:\n",
    "    def __init__(self, use_embeddings=True, relevance_threshold=0.5):\n",
    "        self.doc_text = \"\"\n",
    "        self.sentences = []\n",
    "        self.doc_embeddings = None\n",
    "        self.use_embeddings = use_embeddings\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\") if use_embeddings else None\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "        self.gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "        self.gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "        self.generator = pipeline(\"text-generation\", model=self.gpt2_model, tokenizer=self.gpt2_tokenizer)\n",
    "        logging.info(\"SupportBotAgent initialized.\")\n",
    "\n",
    "    # Load Document\n",
    "    def load_document(self, path):\n",
    "        text = \"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    page_text = \" \".join(page_text.split())  # clean line breaks\n",
    "                    text += page_text + \" \"\n",
    "        self.doc_text = text.strip()\n",
    "\n",
    "        if self.use_embeddings and text.strip():\n",
    "            sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "            self.sentences = sentences\n",
    "            self.doc_embeddings = self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        logging.info(\"Document loaded: %s with %d characters\", path, len(text))\n",
    "\n",
    "    # Answer Query\n",
    "    def answer_query(self, query):\n",
    "        logging.info(\"Query received: %s\", query)   # log query\n",
    "        if not self.doc_text.strip():\n",
    "            return \"No document loaded.\"\n",
    "\n",
    "        if not self.use_embeddings:\n",
    "            return self.doc_text[:200] + \"...\"\n",
    "\n",
    "    # Encode query & find best context\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        cos_scores = util.cos_sim(query_embedding, self.doc_embeddings)[0]\n",
    "        top_score, top_idx = torch.max(cos_scores, dim=0)\n",
    "\n",
    "    # Relevance check\n",
    "        if top_score < self.relevance_threshold:\n",
    "            return \"I donâ€™t have enough information to answer that.\"\n",
    "\n",
    "    # Use QA model\n",
    "        context = self.sentences[top_idx]\n",
    "        qa_result = self.qa_pipeline(question=query, context=context)\n",
    "        answer = qa_result[\"answer\"].strip()\n",
    "\n",
    "    # If answer is too short, use GPT-2 to expand it\n",
    "        if len(answer) < 20:\n",
    "            prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "            gpt2_output = self.generator(\n",
    "            prompt,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # Extract only the answer portion\n",
    "            if \"Answer:\" in gpt2_output:\n",
    "                gpt2_answer = gpt2_output.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                gpt2_answer = gpt2_output.strip()\n",
    "\n",
    "            return gpt2_answer\n",
    "\n",
    "    # If answer is already good length, just return it\n",
    "        return answer\n",
    "\n",
    "\n",
    "    # Simulate Feedback\n",
    "    def simulate_feedback(self, response):\n",
    "        feedback_options = [\"not helpful\", \"too vague\", \"good\"]\n",
    "        feedback = random.choice(feedback_options)\n",
    "\n",
    "        if feedback == \"not helpful\":\n",
    "            response = response + \" (rephrased for clarity)\"\n",
    "        elif feedback == \"too vague\":\n",
    "            response = response + \" (added more context)\"\n",
    "        # \"good\" -> keep as is\n",
    "\n",
    "        logging.info(\"Feedback: %s | Adjusted Response: %s\", feedback, response)\n",
    "        return response, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c410fc2",
   "metadata": {},
   "source": [
    "Main Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5543f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 61.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: I donâ€™t have enough information to answer that.\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: I donâ€™t have enough information to answer that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 78.23it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: If you're looking for a password to unlock a locked door, you probably don't\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: If you're looking for a password to unlock a locked door, you probably don't\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 58.28it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: You need to provide two different passwords to access each of your accounts.\n",
      "Question: Can I change\n",
      "ðŸ”„ Iteration 1 | Feedback: too vague | New Answer: You need to provide two different passwords to access each of your accounts.\n",
      "Question: Can I change (added more context)\n",
      "ðŸ”„ Iteration 2 | Feedback: too vague | New Answer: You need to provide two different passwords to access each of your accounts.\n",
      "Question: Can I change (added more context) (added more context)\n",
      "ðŸ”„ Iteration 3 | Feedback: too vague | New Answer: You need to provide two different passwords to access each of your accounts.\n",
      "Question: Can I change (added more context) (added more context) (added more context)\n",
      "ðŸ”„ Iteration 4 | Feedback: good | New Answer: You need to provide two different passwords to access each of your accounts.\n",
      "Question: Can I change (added more context) (added more context) (added more context)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: I donâ€™t have enough information to answer that.\n",
      "ðŸ”„ Iteration 1 | Feedback: too vague | New Answer: I donâ€™t have enough information to answer that. (added more context)\n",
      "ðŸ”„ Iteration 2 | Feedback: too vague | New Answer: I donâ€™t have enough information to answer that. (added more context) (added more context)\n",
      "ðŸ”„ Iteration 3 | Feedback: not helpful | New Answer: I donâ€™t have enough information to answer that. (added more context) (added more context) (rephrased for clarity)\n",
      "ðŸ”„ Iteration 4 | Feedback: good | New Answer: I donâ€™t have enough information to answer that. (added more context) (added more context) (rephrased for clarity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.73it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: Well, there are two different things that you can do. You can take out money and leave the country and you can loan money out to other people. And you can also take out money and leave the country and you can leave the country and take out money and leave the country. And there is no need for you to do that. Now, what about the other bank accounts? So, what you can do is you could go to another bank account or you could take out money out of the bank and you can then return to the country. Now, what about the other bank accounts? Do you want to go to another bank account? No, you don't. And what about the other bank accounts? Do you want to go to a different bank account? No, you don't. And what about the other bank accounts? You can't go to a bank account without permission. And what about the other bank accounts? So, now I see what you mean. Now you can't go to another bank account without permission. So, you can't take out money from a bank account without permission. Now, what\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: Well, there are two different things that you can do. You can take out money and leave the country and you can loan money out to other people. And you can also take out money and leave the country and you can leave the country and take out money and leave the country. And there is no need for you to do that. Now, what about the other bank accounts? So, what you can do is you could go to another bank account or you could take out money out of the bank and you can then return to the country. Now, what about the other bank accounts? Do you want to go to another bank account? No, you don't. And what about the other bank accounts? Do you want to go to a different bank account? No, you don't. And what about the other bank accounts? You can't go to a bank account without permission. And what about the other bank accounts? So, now I see what you mean. Now you can't go to another bank account without permission. So, you can't take out money from a bank account without permission. Now, what\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 71.60it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Answer: Yes, no charge for this service,\n",
      "Question: Are there any charges\n",
      "ðŸ”„ Iteration 1 | Feedback: good | New Answer: Yes, no charge for this service,\n",
      "Question: Are there any charges\n",
      "\n",
      "ðŸ™ Thanks for using the Support Bot!\n",
      "âœ¨ Hope I was able to help you today.\n",
      "ðŸ’¬ Goodbye, and have a great day ahead! ðŸš€\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = SupportBotAgent()\n",
    "\n",
    "    # File picker: choose ANY PDF from your PC\n",
    "    Tk().withdraw()  # hide empty Tk window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a PDF file\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\")]\n",
    "    )\n",
    "\n",
    "    if not file_path:\n",
    "        print(\"âŒ No file selected.\")\n",
    "    else:\n",
    "        agent.load_document(file_path)\n",
    "        print(\"âœ… Document loaded successfully!\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"\\nAsk a question (or type 'exit'): \")\n",
    "            if query.lower() == \"exit\":\n",
    "                print(\"\\nðŸ™ Thanks for using the Support Bot!\")\n",
    "                print(\"âœ¨ Hope I was able to help you today.\")\n",
    "                print(\"ðŸ’¬ Goodbye, and have a great day ahead! ðŸš€\\n\")\n",
    "                break\n",
    "\n",
    "    # Step 1: Answer query\n",
    "            answer = agent.answer_query(query)\n",
    "            print(\"ðŸ’¡ Answer:\", answer)\n",
    "\n",
    "    # Step 2: Refine with feedback (up to 10 iterations)\n",
    "            for i in range(10):\n",
    "                adjusted, fb = agent.simulate_feedback(answer)\n",
    "                print(f\"ðŸ”„ Iteration {i+1} | Feedback: {fb} | New Answer: {adjusted}\")\n",
    "                answer = adjusted\n",
    "                if fb == \"good\":\n",
    "                    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
